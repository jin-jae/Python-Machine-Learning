{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 텍스트 사전 준비 작업 (텍스트 전처리) - 텍스트 정규화\n",
    "\n",
    "### 클렌징\n",
    "분석에 불필요한 문자 기호 등을 사전에 제거하는 작업\n",
    "\n",
    "### 텍스트 토큰화\n",
    "문장 토큰화 (sentence tokenization): 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리\n",
    "NLTK의 sent_tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jinjae/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text_sample = \"The matrix is everywhere its all around us, here even in this room. You can see it out your window or on your television. You feel it when you go to work, or go to church or pay your taxes.\"\n",
    "sentences = sent_tokenize(text_sample)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "단어 토큰화 (word tokenization): 문장을 단어로 토큰화"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 스톱 워드 제거\n",
    "\n",
    "분석에 큰 의미가 없는 단어 제거"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jinjae/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English stop words counts: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself']\n"
     ]
    }
   ],
   "source": [
    "print(\"English stop words counts:\", len(nltk.corpus.stopwords.words(\"english\")))\n",
    "print(nltk.corpus.stopwords.words(\"English\")[:30])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "all_tokens = []\n",
    "\n",
    "# previous sample\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        # to lowercase\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming 과 Lemmatization\n",
    "\n",
    "과거/현재, 3인칭 단수 여부, 진행형 등에 따라 단어 변화 → 단어의 원형을 찾는 작업\n",
    "\n",
    "Lemmatization은 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음\n",
    "\n",
    "Stemming\n",
    "\n",
    "- 일부 철자가 훼손된 어근 단어 추출\n",
    "- NLTK: Porter, Lancaster, Snowball Stemmer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"working\"), stemmer.stem(\"works\"), stemmer.stem(\"worked\"))\n",
    "print(stemmer.stem(\"amusing\"), stemmer.stem(\"amuses\"), stemmer.stem(\"amused\"))\n",
    "print(stemmer.stem(\"happier\"), stemmer.stem(\"happiest\"))\n",
    "print(stemmer.stem(\"fancier\"), stemmer.stem(\"fanciest\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lemmatization\n",
    "\n",
    "- 문법적인 요소와 의미적인 부분을 감안 (시간이 더 걸림)\n",
    "- NLTK: WordNetLemmatizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/jinjae/nltk_data...\n",
      "[nltk_data] Downloading package wordnet to /Users/jinjae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize(\"amusing\", 'v'), lemma.lemmatize(\"amuses\", 'v'), lemma.lemmatize(\"amused\", 'v'))\n",
    "print(lemma.lemmatize(\"happier\", 'a'), lemma.lemmatize(\"happiest\", 'a'))\n",
    "print(lemma.lemmatize(\"fancier\", 'a'), lemma.lemmatize(\"fanciest\", 'a'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BOW (Bag of Words)\n",
    "\n",
    "문맥 순서를 무시하고 모든 단어에 대해 빈도 값을 부여해 피처 값 추출\n",
    "\n",
    "1. 문장 내 모든 단어에서 중복을 제거하고 인덱스 부여\n",
    "2. 개별 문장에서 해당 단어가 나타내는 횟수를 단어에 기재\n",
    "\n",
    "단점\n",
    "\n",
    "- 문맥 의미 반영 부족: 단어의 문맥적인 해석을 처리하지 못함\n",
    "- 희소 행렬 문제: 대규모 칼럼 중 대부분의 값이 0으로 채워지는 행렬의 발생 (희소 행렬은 알고리즘 수행 시간과 예측 성능을 떨어트림)\n",
    "\n",
    "### BOW 피처 벡터화\n",
    "\n",
    "텍스트와 같은 데이터는 알고리즘에 바로 입력할 수 없음 → 벡터 값으로 변환\n",
    "\n",
    "- 카운트 기반 벡터화\n",
    "    - 각 문서에서 해당 단어가 나타나는 횟수를 부여\n",
    "- TF-IDF 기반 벡터화\n",
    "    - 개별 문서에서 자주 나타나는 단어에 높은 가중치 부여, 모든 문서에서 전반적으로 나타나는 단어에 패널티 부여\n",
    "\n",
    "문서마다 텍스트가 길고 개수가 많은 경우 TF-IDF가 더 좋은 예측 성능 보장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 사이킷런 CountVectorizer, TfidfVectorizer\n",
    "\n",
    "CountVectorizer 파라미터\n",
    "\n",
    "- max_df: 너무 높은 빈도수를 가진 단어 피처 제외\n",
    "- min_df: 너무 낮은 빈도수를 가진 단어 피처 제외\n",
    "- max_features: 가장 높은 것부터 개수를 제한\n",
    "- stop_words: ‘english’로 지정하면 영어에서 스톱 워드 제외\n",
    "- n_gram_range: n_gram 범위 설정\n",
    "- analyzer: 피처 추출을 수행한 단위 지정\n",
    "- token_pattern: 토큰화를 수행하는 정규 표현식 패턴 지정\n",
    "- tokenizer: 토큰화를 별도 커스텀 함수로 이용할 때 적용\n",
    "\n",
    "수행 과정\n",
    "\n",
    "1. 소문자 변경 등 전처리\n",
    "2. n_gram_range 반영 단어 토큰화\n",
    "3. 텍스트 정규화 수행\n",
    "4. 토큰화된 단어 피처로 추출 및 벡터화\n",
    "\n",
    "TfidfVectorizer 또한 거의 유사"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BOW 벡터화를 위한 희소 행렬\n",
    "\n",
    "CountVectorizer / TfidfVectorizer\n",
    "\n",
    "텍스트를 피처 단위로 벡터화해 변환 → CSR 형태의 희소 행렬 반환\n",
    "\n",
    "대규모 행렬(수만 수십 개) 중에서 각 문서가 가지는 단어 수는 제한적이므로 행렬 값은 대부분 0 ⇒ 희소 행렬\n",
    "\n",
    "메모리 공간 많이 필요, 데이터 엑세스 시간 다소 소모\n",
    "\n",
    "COO, CSR 형식으로 변환"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 희소 행렬 - COO 형식\n",
    "\n",
    "COO (Coordinate) 형식: 0이 아닌 데이터만 별도의 데이터 배열에 저장하고 행과 열의 위치를 별도의 배열로 저장\n",
    "\n",
    "Scipy 이용"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([ [3, 0, 1], [0, 2, 0] ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[3, 0, 1],\n       [0, 2, 0]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 희소 행렬 - CSR 형식\n",
    "\n",
    "행 위치 배열의 고유한 값의 시작 위치만 표기 + 행 위치 배열의 총 항목 개수"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                  [1, 4, 0, 3, 2, 5],\n",
    "                  [0, 6, 0, 3, 0, 0],\n",
    "                  [2, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 7, 0, 8],\n",
    "                  [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print(sparse_coo.toarray())\n",
    "print(sparse_csr.toarray())\n",
    "assert(sparse_coo.toarray().all() == sparse_csr.toarray().all())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
